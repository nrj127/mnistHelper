\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\graphicspath{ {images/}}
\usepackage[ngerman,english]{babel}
\usepackage{tabularx}  % for 'tabularx' environment and 'X' column type
\usepackage{ragged2e}  % for '\RaggedRight' macro (allows hyphenation)
\newcolumntype{Y}{>{\RaggedRight\arraybackslash}X} 
\usepackage{float}
\restylefloat{table}

%	options include 12pt or 11pt or 10pt
%	classes include article, report, book, letter, thesis

\title{Machine Learning for Applications in Computer Vision: Week 1}
\author{Neeraj Sujan - 03656452 }
\date{23 April 2015}

\begin{document}
\maketitle

Exercise 2: Support Vector Machines
\\

Results:

The training data set is stored in images and labels. The test data set is stored in images1 and labels1. The tests were carried out for the first 1000 samples using linear kernel,polynomial kernel and RBF kernel. The results of the tests are given below. 

I got the best performance using the polynomial and linear kernel whereas for the RBF kernel, I got the worst performance. The performance metric is as displayed below \\

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline 
Classifier & LinearSVC & SVC,linear kernel & SVC,polynomial kernel & SVC,RBF kernel \\ 
\hline 
Accuracy & 0.8235 & 0.8758 & 0.8865 & 0.1028 \\ 
\hline 
\end{tabular} 
\end{table}

Linear SVC
\begin{figure}[h]
\centering
\includegraphics[width=10cm, height=4cm]{1}
\end{figure}


Linear Kernel
\begin{figure}[h]
\centering
\includegraphics[width=10cm, height=4cm]{2}
\end{figure}

Polynomial kernel
\begin{figure}[h]
\centering
\includegraphics[width=10cm, height=4cm]{3}
\end{figure}

RBF kernel
\begin{figure}[h]
\centering
\includegraphics[width=10cm, height=4cm]{4}
\end{figure}

\newpage

Exercise 3: Decision Tree Classifier \\

The accuracy of the decision tree classifier using default parameters is 0.8763. The following table gives the precision of each and every label.

\begin{tabular}{|c|c|}
\hline 
Labels & Precision \\ 
\hline 
0 & 0.92 \\ 
\hline 
1 & 0.96 \\ 
\hline 
2 & 0.87 \\ 
\hline 
3 & 0.82 \\ 
\hline 
4 & 0.88 \\ 
\hline 
5 & 0.83 \\ 
\hline 
6 & 0.89 \\ 
\hline 
7 & 0.92 \\ 
\hline 
8 & 0.82 \\ 
\hline 
9 & 0.84 \\ 
\hline 
\end{tabular} 
\\

For the following attributes, $criterion='gini',max depth=12, max features=370$, I got the best performance of 0.8791.

The figure below shows the pixel importance.

\begin{figure}[h]
\centering
\includegraphics[width=10cm, height=4cm]{5}
\end{figure}


Cross Validation: Using the same values, for cross-validation, I got a mean score of 0.87\\

Exercise 3 : RandomForestClassifier

The accuracy of the random forest classifier using default parameters is 0.9463. The following table gives the precision of each and every label.

\begin{tabular}{|c|c|}
\hline 
Labels & Precision \\ 
\hline 
0 & 0.95 \\ 
\hline 
1 & 0.98 \\ 
\hline 
2 & 0.94 \\ 
\hline 
3 & 0.91 \\ 
\hline 
4 & 0.94 \\ 
\hline 
5 & 0.93 \\ 
\hline 
6 & 0.97 \\ 
\hline 
7 & 0.96 \\ 
\hline 
8 & 0.94 \\ 
\hline 
9 & 0.93 \\ 
\hline 
\end{tabular} 
\\

For the following attributes, $n estimators =25 ,criterion ="gini",max depth =12,max features=30$, I got the best performance of 0.9539.

The figure below shows the pixel importance.

\begin{figure}[h]
\centering
\includegraphics[width=10cm, height=4cm]{6}
\end{figure}


Cross Validation: Using the same values, for cross-validation, I got a mean score of 0.94 \\

Conclusion: Random Forest Classifier gives a better performance in classifying the digits correctly.

\end{document}
